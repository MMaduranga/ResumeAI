{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOaL+pJYW2YQWGLXL8aBTHU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["%pip install langchain PPyPDF2 faiss-cpu tiktoken"],"metadata":{"id":"K4wF8lnxb87p"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HSSQ981HQRu8"},"outputs":[],"source":["import os\n","import pickle\n","import langchain\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","from langchain import HuggingFaceHub\n","from langchain.chains import RetrievalQA\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.document_loaders import UnstructuredURLLoader\n","from langchain.vectorstores import FAISS\n","\n","#load Hugging Face api key\n","os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"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\"\n","\n","from google.colab import userdata\n","userdata.get('Huggingfacee')\n","\n","\n","llm = HuggingFaceHub(\n","    repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n","    model_kwargs={\"temperature\":0.8, \"max_length\":1000}\n",")\n","\n","loaders = UnstructuredURLLoader(urls=[\n","    \"https://www.aljazeera.com/news/2024/3/29/what-is-openais-sora-text-to-video-generator\"\n","])\n","data = loaders.load()\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,\n","    chunk_overlap=200\n",")\n","\n","# As data is of type documents we can directly use split_documents over split_text in order to get the chunks.\n","docs = text_splitter.split_documents(data)\n","\n","# Create the embeddings of the chunks using openAIEmbeddings\n","embeddings = HuggingFaceEmbeddings()\n","\n","# Pass the documents and embeddings inorder to create FAISS vector index\n","vectorindex_openai = FAISS.from_documents(docs, embeddings)\n","\n","# Storing vector index create in local\n","file_path=\"vector_index.pkl\"\n","with open(file_path, \"wb\") as f:\n","    pickle.dump(vectorindex_openai, f)\n","\n","if os.path.exists(file_path):\n","    with open(file_path, \"rb\") as f:\n","        vectorIndex = pickle.load(f)\n","chain = RetrievalQA.from_chain_type(llm=llm,\n","                                 retriever=vectorIndex.as_retriever(),\n","                                 return_source_documents=True)\n","\n","query = \"Is that possible to create 30 minutes video using SORA?\"\n","\n","langchain.debug=True\n","result = chain({\"query\": query}, return_only_outputs=True)\n","\n","text = result['result']\n","answer_start = text.find(\"Answer:\")\n","\n","# Extract the text after \"Answer:\" until a blank line is found\n","answer_text = text[answer_start:text.find(\"\\n\\n\", answer_start)]\n","\n","# Print the extracted text\n","print(answer_text)\n"]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n","\n","messages = [\n","    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n","        {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n","            {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n","            ]\n","\n","            inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n","\n","            outputs = model.generate(inputs, max_new_tokens=20)\n","            print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"],"metadata":{"id":"0bBVI42Si7Pb"},"execution_count":null,"outputs":[]}]}